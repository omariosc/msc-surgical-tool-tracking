{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Input Multiple Output Anchor-Free Object Detection\n",
    "\n",
    "Code is based off of ART-Net (https://github.com/kamruleee51/ART-Net)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "class ToolDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, transform=None, max_bboxes=2):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transform = transform\n",
    "        self.max_bboxes = max_bboxes\n",
    "        self.image_files = [\n",
    "            f\n",
    "            for f in os.listdir(image_dir)\n",
    "            if os.path.isfile(os.path.join(image_dir, f)) and not \"Neg\" in f and os.path.exists(os.path.join(label_dir, f.replace(\".png\", \".txt\"))) and os.path.getsize(os.path.join(label_dir, f.replace(\".png\", \".txt\")))>1\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        label_path = os.path.join(self.label_dir, img_name.replace(\".png\", \".txt\"))\n",
    "\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.resize(image, (512, 512))  # Resize images to 512x512\n",
    "        image = image / 255.0\n",
    "\n",
    "        if os.path.exists(label_path) and os.path.getsize(label_path) > 0:\n",
    "            labels = np.loadtxt(label_path, delimiter=\" \")\n",
    "            if labels.ndim == 1:\n",
    "                labels = labels[np.newaxis, :]\n",
    "        else:\n",
    "            labels = np.zeros((0, 5))  # No bounding boxes\n",
    "\n",
    "        # Pad labels to fixed size\n",
    "        if len(labels) > self.max_bboxes:\n",
    "            labels = labels[: self.max_bboxes]\n",
    "        else:\n",
    "            padding = np.zeros((self.max_bboxes - len(labels), 5))\n",
    "            labels = np.vstack((labels, padding))\n",
    "\n",
    "        labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, labels\n",
    "\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ToolDataset(\n",
    "    image_dir=\"data/ART-Net/images/train\",\n",
    "    label_dir=\"data/ART-Net/labels/train\",\n",
    "    transform=transform,\n",
    ")\n",
    "val_dataset = ToolDataset(\n",
    "    image_dir=\"data/ART-Net/images/val\",\n",
    "    label_dir=\"data/ART-Net/labels/val\",\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially 1324 train and 308 val images before cleaning negative samples and those with missing bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "662 154\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset), len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import certifi\n",
    "\n",
    "os.environ[\"SSL_CERT_FILE\"] = certifi.where()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "\n",
    "\n",
    "class SIMOModel(nn.Module):\n",
    "    def __init__(self, n_classes=4):  # 4 outputs for bounding box coordinates\n",
    "        super(SIMOModel, self).__init__()\n",
    "        self.vgg_base = models.vgg16(weights=models.VGG16_Weights.DEFAULT).features\n",
    "\n",
    "        # Feature representation generator (FRG)\n",
    "        self.frg = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(64, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.Conv2d(256, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(64, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(512),\n",
    "        )\n",
    "\n",
    "        # Decoder for tool bounding box regression\n",
    "        self.tool_decoder = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                1024, 512, kernel_size=3, padding=1\n",
    "            ),  # 1024 channels due to concatenation of VGG and FRG\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Conv2d(\n",
    "                128, n_classes, kernel_size=1\n",
    "            ),  # 4 outputs for bounding box coordinates\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),  # Pooling to get a 1x1 output\n",
    "            nn.Flatten(),  # Flatten to shape [batch_size, 4]\n",
    "        )\n",
    "\n",
    "        # Decoder for tooltip bounding box regression\n",
    "        self.tooltip_decoder = nn.Sequential(\n",
    "            nn.Conv2d(1024, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Conv2d(\n",
    "                128, n_classes, kernel_size=1\n",
    "            ),  # 4 outputs for bounding box coordinates\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),  # Pooling to get a 1x1 output\n",
    "            nn.Flatten(),  # Flatten to shape [batch_size, 4]\n",
    "        )\n",
    "\n",
    "        # Confidence prediction for tool\n",
    "        self.tool_confidence = nn.Sequential(\n",
    "            nn.Conv2d(1024, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.Conv2d(256, 1, kernel_size=1),\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Sigmoid(),  # Confidence between 0 and 1\n",
    "        )\n",
    "\n",
    "        # Confidence prediction for tooltip\n",
    "        self.tooltip_confidence = nn.Sequential(\n",
    "            nn.Conv2d(1024, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.Conv2d(256, 1, kernel_size=1),\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Sigmoid(),  # Confidence between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # VGG base layers\n",
    "        block1_pool = self.vgg_base[:5](x)  # block1_pool\n",
    "        block2_pool = self.vgg_base[5:10](block1_pool)  # block2_pool\n",
    "        block3_pool = self.vgg_base[10:17](block2_pool)  # block3_pool\n",
    "        block4_pool = self.vgg_base[17:24](block3_pool)  # block4_pool\n",
    "        x_vgg = self.vgg_base[24:](block4_pool)  # block5_pool\n",
    "\n",
    "        # Feature representation generator\n",
    "        frg = self.frg(x)\n",
    "\n",
    "        # Resize FRG output to match VGG output size\n",
    "        frg_resized = F.interpolate(\n",
    "            frg,\n",
    "            size=(x_vgg.size(2), x_vgg.size(3)),\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=True,\n",
    "        )\n",
    "\n",
    "        # Concatenate VGG and FRG outputs for both tool and tooltip\n",
    "        combined_features = torch.cat((x_vgg, frg_resized), dim=1)\n",
    "\n",
    "        # Decoder for tool and tooltip\n",
    "        tool_output = self.tool_decoder(combined_features)\n",
    "        tooltip_output = self.tooltip_decoder(combined_features)\n",
    "\n",
    "        # Confidence predictions\n",
    "        tool_conf = self.tool_confidence(combined_features)\n",
    "        tooltip_conf = self.tooltip_confidence(combined_features)\n",
    "\n",
    "        return tool_output, tool_conf, tooltip_output, tooltip_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(pred, target, smooth=1e-6):\n",
    "    \"\"\"\n",
    "    The Intersection over Union (IoU) also referred to as the Jaccard index (JI),\n",
    "    is essentially a method to quantify the percent overlap between the GT mask\n",
    "    and prediction output. The IoU metric measures the number of pixels common\n",
    "    between the target and prediction masks divided by the total number of pixels\n",
    "    present across both masks.\n",
    "\n",
    "    Input Arguments:\n",
    "        y_true: True Labels of the 2D images so called ground truth (GT).\n",
    "        y_pred: Predicted Labels of the 2D images so called Predicted/ segmented Mask.\n",
    "\n",
    "    Output Arguments:\n",
    "\n",
    "        iou: The IoU between y_true and y_pred\n",
    "\n",
    "    Based on work by;\n",
    "    Author: Md. Kamrul Hasan,\n",
    "            Erasmus Scholar on Medical Imaging and Application (MAIA)\n",
    "            E-mail: kamruleeekuet@gmail.com\n",
    "\n",
    "    \"\"\"\n",
    "    intersection = (pred * target).sum()\n",
    "    union = pred.sum() + target.sum() - intersection\n",
    "    return (intersection + smooth) / (union + smooth)\n",
    "\n",
    "\n",
    "def iou_loss(pred, target):\n",
    "    return 1 - iou(pred, target)\n",
    "\n",
    "\n",
    "def bce_iou_loss(pred, target):\n",
    "    bce_loss = F.binary_cross_entropy_with_logits(pred, target)\n",
    "    iou_loss_value = iou_loss(torch.sigmoid(pred), target)\n",
    "    return bce_loss + iou_loss_value\n",
    "\n",
    "\n",
    "# Focal Loss for confidence\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = F.binary_cross_entropy(inputs, targets, reduction=\"none\")\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1 - pt) ** self.gamma * BCE_loss\n",
    "        return F_loss.mean()\n",
    "\n",
    "\n",
    "focal_loss = FocalLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Snapshot: 0.07573399657294863\n",
      "Epoch 1/2, Loss: 0.07573399657294863\n",
      "Validation Loss: 2.8600229740142824\n",
      "Snapshot: 0.09749921162923177\n",
      "Epoch 2/2, Loss: 0.09749921162923177\n",
      "Validation Loss: 3.7565134525299073\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Move model to the appropriate device\n",
    "model = SIMOModel(n_classes=4).to(device)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 2\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device).float()\n",
    "\n",
    "        # Extract labels for tool and tooltip\n",
    "        tool_labels = (\n",
    "            labels[:, 0, 1:].to(device).float()\n",
    "        )  # bbox labels or segmentation masks\n",
    "        tooltip_labels = (\n",
    "            labels[:, 1, 1:].to(device).float()\n",
    "        )  # bbox labels or segmentation masks\n",
    "        tool_targets = (\n",
    "            labels[:, 0, 0].unsqueeze(1).to(device).float()\n",
    "        )  # confidence targets\n",
    "        tooltip_targets = (\n",
    "            labels[:, 1, 0].unsqueeze(1).to(device).float()\n",
    "        )  # confidence targets\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        tool_preds, tool_conf, tooltip_preds, tooltip_conf = model(images)\n",
    "\n",
    "        # Handle negative examples\n",
    "        if tool_labels.size(0) > 0:\n",
    "            loss_tool = bce_iou_loss(tool_preds, tool_labels)\n",
    "            conf_loss_tool = focal_loss(tool_conf, tool_targets)\n",
    "        else:\n",
    "            conf_loss_tool = focal_loss(\n",
    "                tool_conf, torch.zeros_like(tool_conf, device=device)\n",
    "            )\n",
    "\n",
    "        if tooltip_labels.size(0) > 0:\n",
    "            loss_tooltip = bce_iou_loss(tooltip_preds, tooltip_labels)\n",
    "            conf_loss_tooltip = focal_loss(tooltip_conf, tooltip_targets)\n",
    "        else:\n",
    "            conf_loss_tooltip = focal_loss(\n",
    "                tooltip_conf, torch.zeros_like(tooltip_conf, device=device)\n",
    "            )\n",
    "\n",
    "        loss = loss_tool + loss_tooltip + conf_loss_tool + conf_loss_tooltip\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        print(f\"Snapshot: {running_loss/len(train_loader)}\")\n",
    "        break\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device).float()\n",
    "\n",
    "            # Extract labels for tool and tooltip\n",
    "            tool_labels = (\n",
    "                labels[:, 0, 1:].to(device).float()\n",
    "            )  # bbox labels or segmentation masks\n",
    "            tooltip_labels = (\n",
    "                labels[:, 1, 1:].to(device).float()\n",
    "            )  # bbox labels or segmentation masks\n",
    "            tool_targets = (\n",
    "                labels[:, 0, 0].unsqueeze(1).to(device).float()\n",
    "            )  # confidence targets\n",
    "            tooltip_targets = (\n",
    "                labels[:, 1, 0].unsqueeze(1).to(device).float()\n",
    "            )  # confidence targets\n",
    "\n",
    "            # Forward pass\n",
    "            tool_preds, tool_conf, tooltip_preds, tooltip_conf = model(images)\n",
    "\n",
    "            # Handle negative examples\n",
    "            if tool_labels.size(0) > 0:\n",
    "                loss_tool = bce_iou_loss(tool_preds, tool_labels)\n",
    "                conf_loss_tool = focal_loss(tool_conf, tool_targets)\n",
    "            else:\n",
    "                conf_loss_tool = focal_loss(\n",
    "                    tool_conf, torch.zeros_like(tool_conf, device=device)\n",
    "                )\n",
    "\n",
    "            if tooltip_labels.size(0) > 0:\n",
    "                loss_tooltip = bce_iou_loss(tooltip_preds, tooltip_labels)\n",
    "                conf_loss_tooltip = focal_loss(tooltip_conf, tooltip_targets)\n",
    "            else:\n",
    "                conf_loss_tooltip = focal_loss(\n",
    "                    tooltip_conf, torch.zeros_like(tooltip_conf, device=device)\n",
    "                )\n",
    "\n",
    "            loss = loss_tool + loss_tooltip + conf_loss_tool + conf_loss_tooltip\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    print(f\"Validation Loss: {val_loss/len(val_loader)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
