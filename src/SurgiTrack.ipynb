{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool Detection with YOLOv8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.9185486, 133.97276, 915.95306, 467.31335, 0.35837874, 43.0]]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ultralytics import YOLOv10\n",
    "import supervision as sv\n",
    "\n",
    "model_yolo = YOLOv10(\"chkpts/YOLOv10/yolov10x.pt\")\n",
    "\n",
    "\n",
    "def detect_tools(image):\n",
    "    results = model_yolo(image, verbose=False)\n",
    "    dets = []\n",
    "    for i in range(len(results[0].boxes.xyxy)):\n",
    "        x1, y1, x2, y2 = results[0].boxes.xyxy.numpy()[i].flatten()\n",
    "        conf = results[0].boxes.conf.numpy()[i].flatten()[0]\n",
    "        cls = results[0].boxes.cls.numpy()[i].flatten()[0]\n",
    "        dets.append([x1, y1, x2, y2, conf, cls])\n",
    "    return dets\n",
    "\n",
    "detect_tools(\"data/ART-Net/Train/Train_Positive/Train_Pos_sample_0010.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with SSL Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import certifi\n",
    "\n",
    "os.environ[\"SSL_CERT_FILE\"] = certifi.where()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direction Estimation with Siamese Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class ResNetSiamese(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super(ResNetSiamese, self).__init__()\n",
    "        self.resnet = models.resnet50(pretrained=pretrained)\n",
    "        self.resnet.fc = nn.Identity()  # Remove the final fully connected layer\n",
    "\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=512, num_heads=8)\n",
    "        self.fc_direction = nn.Linear(512, 4)  # Assuming 4 direction classes\n",
    "        self.fc_operator = nn.Linear(\n",
    "            512 + 7, 4\n",
    "        )  # Assuming 7 tool categories and 4 operators\n",
    "\n",
    "    def forward(self, x, category):\n",
    "        batch_size = x.size(0)\n",
    "        features = self.resnet(x)\n",
    "        features = features.view(batch_size, -1, 512)  # Reshape for attention\n",
    "        attn_output, _ = self.attention(features, features, features)\n",
    "\n",
    "        # Direction estimation\n",
    "        direction_output = self.fc_direction(attn_output.mean(dim=1))\n",
    "\n",
    "        # Operator estimation\n",
    "        category_one_hot = torch.nn.functional.one_hot(category, num_classes=7).float()\n",
    "        operator_input = torch.cat((attn_output.mean(dim=1), category_one_hot), dim=1)\n",
    "        operator_output = self.fc_operator(operator_input)\n",
    "\n",
    "        return direction_output, operator_output\n",
    "\n",
    "\n",
    "model_siamese = ResNetSiamese()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_direction_estimator(model, dataloader, criterion, optimizer, epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for images, labels in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Harmonizing Bipartite Graph Matching (HBGM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def calculate_cost_matrix(tracklets, detections):\n",
    "    # Example cost calculation using IoU\n",
    "    cost_matrix = np.zeros((len(tracklets), len(detections)))\n",
    "    for i, track in enumerate(tracklets):\n",
    "        for j, det in enumerate(detections):\n",
    "            cost_matrix[i, j] = 1 - iou(track, det)\n",
    "    return cost_matrix\n",
    "\n",
    "\n",
    "def iou(box1, box2):\n",
    "    # Calculate Intersection over Union (IoU) between two bounding boxes\n",
    "    x1, y1, x2, y2, _, _ = box1\n",
    "    x1_, y1_, x2_, y2_, _, _ = box2\n",
    "    xi1, yi1 = max(x1, x1_), max(y1, y1_)\n",
    "    xi2, yi2 = min(x2, x2_), min(y2, y2_)\n",
    "    inter_area = max(0, xi2 - xi1) * max(0, yi2 - yi1)\n",
    "    box1_area = (x2 - x1) * (y2 - y1)\n",
    "    box2_area = (x2_ - x1_) * (y2_ - y1_)\n",
    "    union_area = box1_area + box2_area - inter_area\n",
    "    return inter_area / union_area\n",
    "\n",
    "\n",
    "def bipartite_graph_matching(tracklets, detections):\n",
    "    cost_matrix = calculate_cost_matrix(tracklets, detections)\n",
    "    # Use Hungarian algorithm or another method to solve bipartite matching\n",
    "    from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "    return row_ind, col_ind\n",
    "\n",
    "\n",
    "def update_tracklets(tracklets, detections, row_ind, col_ind):\n",
    "    for r, c in zip(row_ind, col_ind):\n",
    "        tracklets[r] = detections[c]\n",
    "    return tracklets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Pipeline Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "\n",
    "def process_video(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    tracklets = []  # Initialize empty tracklets\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        detections = detect_tools(frame)  # Step 1: Detect tools\n",
    "\n",
    "        if tracklets:\n",
    "            row_ind, col_ind = bipartite_graph_matching(\n",
    "                tracklets, detections\n",
    "            )  # Step 3: Match tracklets with detections\n",
    "            tracklets = update_tracklets(\n",
    "                tracklets, detections, row_ind, col_ind\n",
    "            )  # Update tracklets\n",
    "        else:\n",
    "            tracklets = detections\n",
    "\n",
    "        # Visualization or further processing...\n",
    "        for det in detections:\n",
    "            x1, y1, x2, y2, conf, _ = det\n",
    "            # Make all points integers\n",
    "            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f\"{conf:.2f}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n",
    "\n",
    "        cv2.imshow(\"Frame\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running using a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from natsort import natsorted\n",
    "\n",
    "\n",
    "def preprocess_image(image, bbox):\n",
    "    x, y, w, h = bbox\n",
    "    cropped_image = image[y : y + h, x : x + w]\n",
    "    padded_image = np.pad(cropped_image, ((10, 10), (10, 10), (0, 0)), \"constant\")\n",
    "    resized_image = cv2.resize(padded_image, (224, 224))\n",
    "    return resized_image\n",
    "\n",
    "\n",
    "def process_directory(directory_path):\n",
    "    image_files = [\n",
    "        f for f in os.listdir(directory_path) if f.endswith((\".png\", \".jpg\", \".jpeg\"))\n",
    "    ]\n",
    "    image_files = natsorted(image_files)[:100]\n",
    "    tracklets = []\n",
    "    frame_array = []\n",
    "\n",
    "    for image_file in image_files:\n",
    "        frame = cv2.imread(os.path.join(directory_path, image_file))\n",
    "        detections = detect_tools(frame)\n",
    "\n",
    "        if tracklets:\n",
    "            row_ind, col_ind = bipartite_graph_matching(tracklets, detections)\n",
    "            tracklets = update_tracklets(tracklets, detections, row_ind, col_ind)\n",
    "        else:\n",
    "            tracklets = detections\n",
    "\n",
    "        # Preprocess and predict direction and operator\n",
    "        for det in detections:\n",
    "            x1, y1, x2, y2, conf, _ = det\n",
    "            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.putText(\n",
    "                frame,\n",
    "                f\"{conf:.2f}\",\n",
    "                (x1, y1 - 10),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                0.9,\n",
    "                (36, 255, 12),\n",
    "                2,\n",
    "            )\n",
    "\n",
    "        frame_array.append(frame)\n",
    "\n",
    "    height, width, layers = frame_array[0].shape\n",
    "    size = (width, height)\n",
    "    out = cv2.VideoWriter(\"output.mp4\", cv2.VideoWriter_fourcc(*\"mp4v\"), 30, size)\n",
    "\n",
    "    for frame in frame_array:\n",
    "        out.write(frame)\n",
    "    out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_pipeline(input_path):\n",
    "    if os.path.isdir(input_path):\n",
    "        process_directory(input_path)\n",
    "    else:\n",
    "        process_video(input_path)\n",
    "\n",
    "\n",
    "main_pipeline(\"data/ART-Net/Train/Train_Positive\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
