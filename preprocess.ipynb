{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise Loss and mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "losses = []\n",
    "accuracies = []\n",
    "\n",
    "# Losses and accurances are stored in the results.csv file in the save_dir\n",
    "with open(\"chkpts/ART/yolov8x-semiseg-artnet/results.csv\") as file:\n",
    "    lines = file.readlines()\n",
    "    losses = [float(line.split(\",\")[1]) for line in lines[1:]]\n",
    "    accuracies = [float(line.split(\",\")[7]) for line in lines[1:]]    \n",
    "\n",
    "# append artnet2 results\n",
    "with open(\"chkpts/ART/yolov8x-semiseg-artnet2/results.csv\") as file:\n",
    "    lines = file.readlines()\n",
    "    losses += [float(line.split(\",\")[1]) for line in lines[1:]]\n",
    "    accuracies += [float(line.split(\",\")[7]) for line in lines[1:]]\n",
    "\n",
    "# append artnet4 results\n",
    "with open(\"chkpts/ART/yolov8x-semiseg-artnet4/results.csv\") as file:\n",
    "    lines = file.readlines()\n",
    "    losses += [float(line.split(\",\")[1]) for line in lines[1:]]\n",
    "    accuracies += [float(line.split(\",\")[7]) for line in lines[1:]]\n",
    "    \n",
    "# Plot the training loss and validation mAP\n",
    "plt.figure(figsize=(10, 5))\n",
    "# Epoch starts at 1 and are all integers\n",
    "epochs = range(1, len(losses) + 1)\n",
    "epochs = [int(epoch) for epoch in epochs]\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.xticks(epochs[::10], rotation=45)\n",
    "plt.plot(epochs, losses)\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.xticks(epochs[::10], rotation=45)\n",
    "plt.plot(epochs, accuracies)\n",
    "plt.title(\"Validation mAP\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"mAP\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"results/ART/metrics.png\")\n",
    "plt.show()\n",
    "\n",
    "# output best accuracy and epoch\n",
    "best_accuracy = max(accuracies)\n",
    "best_epoch = accuracies.index(best_accuracy) + 1\n",
    "print(f\"Best accuracy: {best_accuracy} at epoch {best_epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# Define paths\n",
    "base_path = \"D:/Data/PETRAW/\"\n",
    "# folders = [\"images\", \"labels\", \"masks\", \"tool_tip_masks\"]\n",
    "folders = [\"images\", \"labels\"]\n",
    "train_folder = \"test\"\n",
    "val_folder = \"one_test\"\n",
    "val_split = 0.01\n",
    "\n",
    "# Create validation folders if they do not exist\n",
    "for folder in folders:\n",
    "    os.makedirs(os.path.join(base_path, folder, val_folder), exist_ok=True)\n",
    "\n",
    "# Get list of all files in the train folders (using images as reference)\n",
    "train_images_path = os.path.join(base_path, \"images\", train_folder)\n",
    "all_files = [\n",
    "    f\n",
    "    for f in os.listdir(train_images_path)\n",
    "    if os.path.isfile(os.path.join(train_images_path, f))\n",
    "]\n",
    "\n",
    "# Determine the number of validation samples\n",
    "num_val_samples = int(len(all_files) * val_split)\n",
    "print(len(all_files))\n",
    "# Randomly select files for the validation set\n",
    "val_files = random.sample(all_files, num_val_samples)\n",
    "\n",
    "# Move selected files to the validation folders\n",
    "for folder in folders:\n",
    "    train_path = os.path.join(base_path, folder, train_folder)\n",
    "    val_path = os.path.join(base_path, folder, val_folder)\n",
    "\n",
    "    for file in val_files:\n",
    "        file_name, file_ext = os.path.splitext(file)\n",
    "\n",
    "        # Find corresponding file (allowing for different extensions)\n",
    "        corresponding_file = None\n",
    "        for ext in [\".png\", \".npy\", \".txt\"]:\n",
    "            if os.path.exists(os.path.join(train_path, file_name + ext)):\n",
    "                corresponding_file = file_name + ext\n",
    "                break\n",
    "\n",
    "        if corresponding_file:\n",
    "            shutil.move(\n",
    "                os.path.join(train_path, corresponding_file),\n",
    "                os.path.join(val_path, corresponding_file),\n",
    "            )\n",
    "\n",
    "print(\"Test set created with {} samples.\".format(num_val_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "56473 in val first\n",
    "567 in 1% data in initial training and validation and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined Data Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "dataset_path = \"D:/Data/Combined\"\n",
    "\n",
    "# Create a configuration file for YOLOv8\n",
    "config_content = f\"\"\"\n",
    "datasets: \n",
    "train: {dataset_path}/images/train\n",
    "val: {dataset_path}/images/val\n",
    "\n",
    "nc: 2  # number of classes\n",
    "names: ['tool', 'tool']  # class names\n",
    "\"\"\"\n",
    "\n",
    "# config_content = f\"\"\"\n",
    "# train: {dataset_path}/images/train\n",
    "# val: {dataset_path}/images/val\n",
    "\n",
    "# nc: 1  # number of classes\n",
    "# names: ['tool']  # class names\n",
    "# \"\"\"\n",
    "\n",
    "config_path = os.path.join(\"yaml/data-combined.yaml\")\n",
    "with open(config_path, \"w\") as file:\n",
    "    file.write(config_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a configuration file for YOLOv8 test\n",
    "config_content_test = f\"\"\"\n",
    "train: {dataset_path}/images/one\n",
    "val: {dataset_path}/images/one_test\n",
    "\n",
    "nc: 2  # number of classes\n",
    "names: ['left_tool', 'right_tool']  # class names\n",
    "\"\"\"\n",
    "config_path_test = os.path.join(dataset_path, \"data-small-test.yaml\")\n",
    "with open(config_path_test, \"w\") as file:\n",
    "    file.write(config_content_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a configuration file for YOLOv8 test\n",
    "config_content_final = f\"\"\"\n",
    "train: \n",
    "  - {dataset_path}/images/one\n",
    "  - {dataset_path}/images/one_val\n",
    "val: {dataset_path}/images/one_test\n",
    "\n",
    "nc: 2  # number of classes\n",
    "names: ['left_tool', 'right_tool']  # class names\n",
    "\"\"\"\n",
    "config_path_final = os.path.join(dataset_path, \"data-small-final.yaml\")\n",
    "with open(config_path_final, \"w\") as file:\n",
    "    file.write(config_content_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from ultralytics import YOLO\n",
    "from multiprocessing import freeze_support\n",
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.__version__)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "model = YOLO(\"chkpts/ART/yolov8x-semiseg-artnet3/weights/best.pt\")\n",
    "model.to(device)\n",
    "results = model.val(data=config_path_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLO boxes information\n",
    "\n",
    "    Calculates and aggregates detection and segmentation metrics over a given set of classes.\n",
    "\n",
    "    Args:\n",
    "        save_dir (Path): Path to the directory where the output plots should be saved. Default is the current directory.\n",
    "        plot (bool): Whether to save the detection and segmentation plots. Default is False.\n",
    "        on_plot (func): An optional callback to pass plots path and data when they are rendered. Defaults to None.\n",
    "        names (list): List of class names. Default is an empty list.\n",
    "\n",
    "    Attributes:\n",
    "        save_dir (Path): Path to the directory where the output plots should be saved.\n",
    "        plot (bool): Whether to save the detection and segmentation plots.\n",
    "        on_plot (func): An optional callback to pass plots path and data when they are rendered.\n",
    "        names (list): List of class names.\n",
    "        box (Metric): An instance of the Metric class to calculate box detection metrics.\n",
    "        seg (Metric): An instance of the Metric class to calculate mask segmentation metrics.\n",
    "        speed (dict): Dictionary to store the time taken in different phases of inference.\n",
    "\n",
    "    Methods:\n",
    "        process(tp_m, tp_b, conf, pred_cls, target_cls): Processes metrics over the given set of predictions.\n",
    "        mean_results(): Returns the mean of the detection and segmentation metrics over all the classes.\n",
    "        class_result(i): Returns the detection and segmentation metrics of class `i`.\n",
    "        maps: Returns the mean Average Precision (mAP) scores for IoU thresholds ranging from 0.50 to 0.95.\n",
    "        fitness: Returns the fitness scores, which are a single weighted combination of metrics.\n",
    "        ap_class_index: Returns the list of indices of classes used to compute Average Precision (AP).\n",
    "        results_dict: Returns the dictionary containing all the detection and segmentation metrics and fitness score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.results_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = model.track(\"/Volumes/Exodus/Data/6DOF 2023/Test 1/Dataset.mp4\", tracker=\"bytetrack.yaml\", save=True, show=True)\n",
    "\n",
    "results = model.track(\n",
    "    \"data/6DOF/Dataset.mp4\",\n",
    "    tracker=\"bytetrack.yaml\",\n",
    "    save=True,\n",
    "    show=True,\n",
    ")\n",
    "\n",
    "# results = model.track(\n",
    "#     \"/Volumes/Exodus/Data/EndoVis 2015/Tracking (Raw Video)/Dataset1/Video.avi\",\n",
    "#     tracker=\"bytetrack.yaml\",\n",
    "#     save=True,\n",
    "#     show=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Extra Boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Function to remove overlapping boxes using Non-Max Suppression\n",
    "def non_max_suppression(boxes, scores, iou_threshold):\n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    boxes = np.array(boxes)\n",
    "    scores = np.array(scores)\n",
    "\n",
    "    # Compute areas of each box\n",
    "    areas = (boxes[:, 2] - boxes[:, 0] + 1) * (boxes[:, 3] - boxes[:, 1] + 1)\n",
    "    order = scores.argsort()[::-1]  # Sort by score in descending order\n",
    "\n",
    "    keep = []\n",
    "    while order.size > 0:\n",
    "        i = order[0]\n",
    "        keep.append(i)\n",
    "        if order.size == 1:\n",
    "            break\n",
    "        xx1 = np.maximum(boxes[i, 0], boxes[order[1:], 0])\n",
    "        yy1 = np.maximum(boxes[i, 1], boxes[order[1:], 1])\n",
    "        xx2 = np.minimum(boxes[i, 2], boxes[order[1:], 2])\n",
    "        yy2 = np.minimum(boxes[i, 3], boxes[order[1:], 3])\n",
    "\n",
    "        w = np.maximum(0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0, yy2 - yy1 + 1)\n",
    "        inter = w * h\n",
    "\n",
    "        iou = inter / (areas[i] + areas[order[1:]] - inter)\n",
    "        inds = np.where(iou <= iou_threshold)[0]\n",
    "        order = order[inds + 1]\n",
    "\n",
    "    return keep\n",
    "\n",
    "\n",
    "def process_image(image_path, file, model, iou_threshold=0.3, confidence_threshold=0.5, max_boxes=2):\n",
    "    # Perform inference on a new image\n",
    "    results = model(image_path, save=False, show=False, verbose=False)\n",
    "\n",
    "    # Extract the results\n",
    "    boxes = results[0].boxes.xyxy.cpu().numpy()\n",
    "    scores = results[0].boxes.conf.cpu().numpy()\n",
    "\n",
    "    # Step 1: Filter out boxes with confidence scores less than the threshold\n",
    "    filtered_indices = np.where(scores >= confidence_threshold)[0]\n",
    "    filtered_boxes = boxes[filtered_indices]\n",
    "    filtered_scores = scores[filtered_indices]\n",
    "\n",
    "    # Step 2: Remove overlapping boxes using Non-Max Suppression\n",
    "    nms_indices = non_max_suppression(filtered_boxes, filtered_scores, iou_threshold)\n",
    "\n",
    "    # Keep only the top boxes\n",
    "    top_indices = nms_indices[:max_boxes]\n",
    "\n",
    "    # Update results with filtered boxes\n",
    "    results[0].boxes = results[0].boxes[top_indices]\n",
    "\n",
    "    # Display the results\n",
    "    # results[0].show()\n",
    "\n",
    "    # Store image in tmp\n",
    "    tmp = \"tmp/\" + file.replace(\".bmp\", \"_tmp.png\")\n",
    "    results[0].save(tmp)\n",
    "\n",
    "# test_dir = \"/Volumes/Exodus/Data/6DOF 2023/Test 1/\"\n",
    "test_dir = \"/Volumes/Exodus/Data/EndoVis 2015/Testing (Raw Images)/OP1/\"\n",
    "\n",
    "# For all bmp files\n",
    "for file in sorted(os.listdir(test_dir)):\n",
    "    if file.endswith(\".png\") and not file.startswith(\"._\"):\n",
    "        test_path = os.path.join(test_dir, file)\n",
    "        process_image(test_path, file, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine bmp files to mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio.v2 as imageio\n",
    "\n",
    "\n",
    "# Combine all bmp files to video\n",
    "# Create video from BMP files\n",
    "def stitch_images(bmp_directory, output_video_path):\n",
    "    # List all BMP files in the directory\n",
    "    bmp_files = sorted(\n",
    "        [\n",
    "            os.path.join(bmp_directory, f)\n",
    "            for f in os.listdir(bmp_directory)\n",
    "            if f.endswith(\".png\") and not f.startswith(\".\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Create a video writer object\n",
    "    with imageio.get_writer(output_video_path, fps=24) as writer:\n",
    "        for file_path in bmp_files:\n",
    "            image = imageio.imread(file_path)\n",
    "            writer.append_data(image)\n",
    "\n",
    "    print(f\"Video saved at {output_video_path}\")\n",
    "\n",
    "\n",
    "output_image_name = \"results/Test 1/yolo8-1.mp4\"\n",
    "\n",
    "# Stitch the images together\n",
    "# stitch_images(\"tmp\", output_image_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot ground truth and model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Use label file to draw original bounding box on original image\n",
    "sample_label = \"/Volumes/Exodus/Data/ART-Net/labels/val/Test_Pos_sample_0001.txt\"\n",
    "sample_image = (\n",
    "    \"/Volumes/Exodus/Data/ART-Net/Test/Test_Positive/Test_Pos_sample_0001.png\"\n",
    ")\n",
    "\n",
    "# Draw plot with two images, one of sample image and one with results[0]\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "image = cv2.imread(sample_image)\n",
    "with open(sample_label, \"r\") as lf:\n",
    "    for line in lf:\n",
    "        label = line.strip().split(\" \")\n",
    "    x_center, y_center, width, height = map(float, label[1:])\n",
    "    image = cv2.imread(sample_image)\n",
    "    x = int((x_center - width / 2) * image.shape[1])\n",
    "    y = int((y_center - height / 2) * image.shape[0])\n",
    "    w = int(width * image.shape[1])\n",
    "    h = int(height * image.shape[0])\n",
    "    cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# plt.subplot(1, 2, 2)\n",
    "# save image\n",
    "# results[0].save(\"tmp/results.png\")\n",
    "# # display image\n",
    "# plt.imshow(cv2.cvtColor(cv2.imread(\"tmp/results.png\"), cv2.COLOR_BGR2RGB))\n",
    "# plt.axis(\"off\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove black border from frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have a video data/6DOF/Dataset.mp4 which has a weird black border around it\n",
    "# I want to crop the video to remove the black border\n",
    "\n",
    "import cv2\n",
    "import numpy\n",
    "import os\n",
    "\n",
    "# Read the video\n",
    "video_path = \"data/6DOF/Dataset.mp4\"\n",
    "\n",
    "if not os.path.exists(\"tmp_img\"):\n",
    "    os.makedirs(\"tmp_img\")\n",
    "    \n",
    "# convert video to images and store in temp_img folder\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "i = 0\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    cv2.imwrite(f\"tmp_img/{i}.png\", frame)\n",
    "    i += 1\n",
    "cap.release()\n",
    "\n",
    "img = cv2.imread(f\"tmp_img/0.png\")\n",
    "# iterate across all 4 sides and find the first non-black pixel\n",
    "top = 0\n",
    "bottom = img.shape[0]\n",
    "left = 0\n",
    "right = img.shape[1]\n",
    "for y in range(0, img.shape[0]):\n",
    "    if not numpy.all(img[y] == 0):\n",
    "        top = y\n",
    "        break\n",
    "for y in range(img.shape[0] - 1, 0, -1):\n",
    "    if not numpy.all(img[y] == 0):\n",
    "        bottom = y\n",
    "        break\n",
    "for x in range(0, img.shape[1]):\n",
    "    if not numpy.all(img[:, x] == 0):\n",
    "        left = x\n",
    "        break\n",
    "for x in range(img.shape[1] - 1, 0, -1):\n",
    "    if not numpy.all(img[:, x] == 0):\n",
    "        right = x\n",
    "        break\n",
    "        \n",
    "# Now for all images, iterate over them and crop the black border in all 4 directions\n",
    "img_array = []\n",
    "for i in range(0, i):\n",
    "    img = cv2.imread(f\"tmp_img/{i}.png\")\n",
    "    # crop the image\n",
    "    img = img[top:bottom, left:right]\n",
    "    # save the image\n",
    "    cv2.imwrite(f\"tmp_img/{i}.png\", img)\n",
    "    height, width, layers = img.shape\n",
    "    size = (width, height)\n",
    "    img_array.append(img)\n",
    "    \n",
    "# Store video at 30 fps\n",
    "out = cv2.VideoWriter(\"data/6DOF/Dataset_cropped.mp4\", cv2.VideoWriter_fourcc(*\"mp4v\"), 30, size)\n",
    "for i in range(len(img_array)):\n",
    "    out.write(img_array[i])\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess same image names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename every image inside each folder D:\\Data\\PETRAW\\Training\\Training\\Images to include its folder name (e.g. 001) to the front (as a string)\n",
    "import os\n",
    "\n",
    "# Path to the folder containing the images\n",
    "path = \"D:/Data/PETRAW/Test/Images/\"\n",
    "# path = \"D:/Data/PETRAW/labels/test/\"\n",
    "labels = \"D:/Data/PETRAW/images/test\"\n",
    "# labels = \"D:/Data/PETRAW/labels/test\"\n",
    "\n",
    "# List all the directories in the folder\n",
    "dirs = os.listdir(path)\n",
    "# filter out .txt\n",
    "dirs = [dir for dir in dirs if not dir.endswith(\".txt\")]\n",
    "print(dirs)\n",
    "# quit()\n",
    "\n",
    "# For each directory\n",
    "for dir in dirs:\n",
    "    # List all the files in the directory\n",
    "    files = os.listdir(path + dir)\n",
    "    # For each file\n",
    "    for file in files:\n",
    "        if file.startswith(\"frame\"):\n",
    "            # make a copy of the file with the directory name in front in labels folder\n",
    "            os.rename(path + dir + \"/\" + file, labels + \"/\" + dir + \"_\" + file)\n",
    "\n",
    "    # delete the directory forcefully\n",
    "    try:\n",
    "        os.rmdir(path + dir)\n",
    "    except OSError as e:\n",
    "        # delete all files beginning with .\n",
    "        files = os.listdir(path + dir)\n",
    "        for file in files:\n",
    "            if file.startswith(\".\"):\n",
    "                os.remove(path + dir + \"/\" + file)\n",
    "        # delete the directory forcefully\n",
    "        os.rmdir(path + dir)\n",
    "    print(f\"Deleted {dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert MP4 to PNG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a script which takes in a mp4 file and converts it to a series of images, labelling each one with {frame_number}.png\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "\n",
    "def convert_to_imgs(path):\n",
    "    # Create a VideoCapture object\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    # Get the number of frames\n",
    "    n = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    # For each frame\n",
    "    for i in range(n):\n",
    "        # Read the frame\n",
    "        ret, frame = cap.read()\n",
    "        # Save the frame as a png image\n",
    "        cv2.imwrite(f\"data/6DOF/Test 1/{i}.png\", frame)\n",
    "        print(f\"Saved frame {i}\")\n",
    "    # Release the VideoCapture object\n",
    "    cap.release()\n",
    "\n",
    "\n",
    "# Path to the video file\n",
    "path = \"data/6DOF/Test 1/Task1_stitched_video.mp4\"\n",
    "convert_to_imgs(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating csv file annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file 'data/ART-Net/train.csv' has been created successfully.\n",
      "CSV file 'data/ART-Net/val.csv' has been created successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import cv2\n",
    "\n",
    "# Paths to the directories containing images and labels\n",
    "image_dirs = {\"train\": \"data/ART-Net/images/train\", \"val\": \"data/ART-Net/images/val\"}\n",
    "label_dirs = {\"train\": \"data/ART-Net/labels/train\", \"val\": \"data/ART-Net/labels/val\"}\n",
    "\n",
    "# Output CSV files\n",
    "output_csv_files = {\"train\": \"data/ART-Net/train.csv\", \"val\": \"data/ART-Net/val.csv\"}\n",
    "\n",
    "\n",
    "def convert_bbox_to_xyxy(cls, cx, cy, w, h, img_width, img_height):\n",
    "    x1 = int((cx - w / 2) * img_width)\n",
    "    y1 = int((cy - h / 2) * img_height)\n",
    "    x2 = int((cx + w / 2) * img_width)\n",
    "    y2 = int((cy + h / 2) * img_height)\n",
    "    return [x1, y1, x2, y2, cls]\n",
    "\n",
    "\n",
    "def process_labels(image_dir, label_dir):\n",
    "    data = []\n",
    "    \n",
    "    id = {0: \"tool\", 1: \"tip\"}\n",
    "\n",
    "    for image_file in os.listdir(image_dir):\n",
    "        if image_file.endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "            # Construct the full image path\n",
    "            image_path = os.path.join(image_dir, image_file)\n",
    "\n",
    "            # Get the corresponding label file\n",
    "            label_file = (\n",
    "                image_file.replace(\".png\", \".txt\")\n",
    "                .replace(\".jpg\", \".txt\")\n",
    "                .replace(\".jpeg\", \".txt\")\n",
    "            )\n",
    "            label_path = os.path.join(label_dir, label_file)\n",
    "\n",
    "            if not os.path.exists(label_path):\n",
    "                continue\n",
    "\n",
    "            # Read the image to get its dimensions\n",
    "            img = cv2.imread(image_path)\n",
    "            img_height, img_width = img.shape[:2]\n",
    "\n",
    "            with open(label_path, \"r\") as lf:\n",
    "                lines = lf.readlines()\n",
    "                for line in lines:\n",
    "                    cls, cx, cy, w, h = map(float, line.strip().split())\n",
    "                    bbox = convert_bbox_to_xyxy(\n",
    "                        cls, cx, cy, w, h, img_width, img_height\n",
    "                    )\n",
    "                    data.append([image_file] + bbox[:4] + [id[int(bbox[4])]])\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# Create CSV files for train and val datasets\n",
    "for key in image_dirs:\n",
    "    data = process_labels(image_dirs[key], label_dirs[key])\n",
    "    output_csv = output_csv_files[key]\n",
    "\n",
    "    with open(output_csv, \"w\", newline=\"\") as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        for row in data:\n",
    "            csvwriter.writerow(row)\n",
    "\n",
    "    print(f\"CSV file '{output_csv}' has been created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON file 'train_dataset.json' has been created successfully.\n",
      "JSON file 'val_dataset.json' has been created successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "\n",
    "# Paths to the directories containing images and labels\n",
    "image_dirs = {\"train\": \"data/ART-Net/images/train\", \"val\": \"data/ART-Net/images/val\"}\n",
    "label_dirs = {\"train\": \"data/ART-Net/labels/train\", \"val\": \"data/ART-Net/labels/val\"}\n",
    "\n",
    "# Output JSON files\n",
    "output_json_files = {\"train\": \"train_dataset.json\", \"val\": \"val_dataset.json\"}\n",
    "\n",
    "# Define the COCO categories\n",
    "categories = [{\"id\": 0, \"name\": \"tool\"}, {\"id\": 1, \"name\": \"tip\"}]\n",
    "\n",
    "\n",
    "def convert_bbox_to_coco_format(cx, cy, w, h, img_width, img_height):\n",
    "    x = int((cx - w / 2) * img_width)\n",
    "    y = int((cy - h / 2) * img_height)\n",
    "    width = int(w * img_width)\n",
    "    height = int(h * img_height)\n",
    "    return [x, y, width, height]\n",
    "\n",
    "\n",
    "def process_labels(image_dir, label_dir):\n",
    "    images = []\n",
    "    annotations = []\n",
    "    annotation_id = 1\n",
    "    image_id = 1\n",
    "\n",
    "    for image_file in os.listdir(image_dir):\n",
    "        if image_file.endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "            # Construct the full image path\n",
    "            image_path = os.path.join(image_dir, image_file)\n",
    "\n",
    "            # Get the corresponding label file\n",
    "            label_file = (\n",
    "                image_file.replace(\".png\", \".txt\")\n",
    "                .replace(\".jpg\", \".txt\")\n",
    "                .replace(\".jpeg\", \".txt\")\n",
    "            )\n",
    "            label_path = os.path.join(label_dir, label_file)\n",
    "\n",
    "            if not os.path.exists(label_path):\n",
    "                continue\n",
    "\n",
    "            # Read the image to get its dimensions\n",
    "            img = cv2.imread(image_path)\n",
    "            img_height, img_width = img.shape[:2]\n",
    "\n",
    "            # Add image info to images list\n",
    "            images.append(\n",
    "                {\n",
    "                    \"file_name\": image_file,\n",
    "                    \"height\": img_height,\n",
    "                    \"width\": img_width,\n",
    "                    \"id\": image_id,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            with open(label_path, \"r\") as lf:\n",
    "                lines = lf.readlines()\n",
    "                for line in lines:\n",
    "                    cls, cx, cy, w, h = map(float, line.strip().split())\n",
    "                    bbox = convert_bbox_to_coco_format(\n",
    "                        cx, cy, w, h, img_width, img_height\n",
    "                    )\n",
    "                    annotations.append(\n",
    "                        {\n",
    "                            \"id\": annotation_id,\n",
    "                            \"image_id\": image_id,\n",
    "                            \"category_id\": int(cls),\n",
    "                            \"bbox\": bbox,\n",
    "                            \"area\": bbox[2] * bbox[3],\n",
    "                            \"iscrowd\": 0,\n",
    "                        }\n",
    "                    )\n",
    "                    annotation_id += 1\n",
    "\n",
    "            image_id += 1\n",
    "\n",
    "    return images, annotations\n",
    "\n",
    "\n",
    "# Create JSON files for train and val datasets\n",
    "for key in image_dirs:\n",
    "    images, annotations = process_labels(image_dirs[key], label_dirs[key])\n",
    "    data_coco = {\n",
    "        \"info\": {\n",
    "            \"description\": \"ART-Net Dataset\",\n",
    "        },\n",
    "        \"images\": images,\n",
    "        \"annotations\": annotations,\n",
    "        \"categories\": categories,\n",
    "    }\n",
    "\n",
    "    output_json = output_json_files[key]\n",
    "\n",
    "    with open(output_json, \"w\") as jsonfile:\n",
    "        json.dump(data_coco, jsonfile)\n",
    "\n",
    "    print(f\"JSON file '{output_json}' has been created successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
